#!/usr/bin/env python3

# Copyright Â© 2022 Jakub Wilk <jwilk@jwilk.net>
# SPDX-License-Identifier: MIT

import argparse
import contextlib
import gzip
import html.parser
import http.client
import inspect
import io
import json
import os
import re
import shutil
import signal
import subprocess
import sys
import textwrap
import types
import urllib.parse
import urllib.request

type(0_0)  # Python >= 3.6 is required

text_width = int(os.getenv('ZYGOLOPHODON_COLUMNS', '78'))

prog = argparse.ArgumentParser().prog

def find_command(command):
    if shutil.which(command):
        return command
    return None

def fatal(msg):
    print(f'{prog}: {msg}', file=sys.stderr)
    sys.exit(1)

class StdOut(io.TextIOBase):

    def _install_pager(self):
        if not sys.__stdout__.isatty():
            return
        cmdline = (os.environ.get('PAGER')
            or find_command('pager')  # Debian:
            # https://www.debian.org/doc/debian-policy/ch-customized-programs.html#editors-and-pagers
            or 'more'  # POSIX:
            # https://pubs.opengroup.org/onlinepubs/007904975/utilities/man.html#tag_04_85_08
        )
        if cmdline == 'cat':
            return
        env = None
        if 'LESS' not in os.environ:
            env = dict(env or os.environ, LESS='-FXK')
        self._pager = subprocess.Popen(cmdline, shell=True, stdin=subprocess.PIPE, env=env)  # pylint: disable=consider-using-with
        self._stdout = io.TextIOWrapper(self._pager.stdin,
            encoding=sys.__stdout__.encoding,
            errors=sys.__stdout__.errors,
            line_buffering=True,
        )

    def __init__(self):
        super().__init__()
        self._newlines = 0
        self._pager = None
        self._stdout = sys.__stdout__
        self._install_pager()

    def _get_fp(self):
        if UserAgent.debug_level:
            # Redirect http.client's debug messages to stderr:
            for frameinfo in inspect.stack(context=0):
                if frameinfo.filename == http.client.__file__:
                    return sys.__stderr__
        return self._stdout

    def write(self, s):
        fp = self._get_fp()
        if fp is self._stdout:
            if s == '':
                return
            if s == '\n':
                if self._newlines == 2:
                    return
                self._newlines += 1
            else:
                self._newlines = int(s[-1] == '\n')
        fp.write(s)

    def flush(self):
        self._get_fp().flush()

    def isatty(self):
        return sys.__stdout__.isatty()

    def __exit__(self, exc_type, exc_value, traceback):
        if self._pager:
            self._pager.__exit__(exc_type, exc_value, traceback)
            if exc_type is None and self._pager.returncode != 0:
                raise RuntimeError('pager failed')
            self._pager = None
            self._stdout = None

    @staticmethod
    @contextlib.contextmanager
    def install():
        assert sys.stdout is sys.__stdout__
        try:
            with StdOut() as sys.stdout:
                yield
        finally:
            sys.stdout = sys.__stdout__

class UserAgent():

    headers = {
        'User-Agent': 'zygolophodon (https://github.com/jwilk/zygolophodon)',
        'Accept-Encoding': 'gzip',
    }
    debug_level = 0

    @classmethod
    def _build_opener(cls):
        # TODO: Get rid of this function once
        # <https://github.com/python/cpython/issues/99352> is fixed.
        handlers = [
            Handler(debuglevel=cls.debug_level)
            for Handler in [urllib.request.HTTPHandler, urllib.request.HTTPSHandler]
        ]
        return urllib.request.build_opener(*handlers)

    @classmethod
    def get(cls, url):
        headers = dict(cls.headers)
        request = urllib.request.Request(url, headers=headers)
        opener = cls._build_opener()
        response = opener.open(request)
        return Response(response)

class Response():

    def __init__(self, response):
        with response:
            content_encoding = response.getheader('Content-Encoding', 'identity')
            data = response.read()
        if content_encoding == 'gzip':
            data = gzip.decompress(data)
        elif content_encoding == 'identity':
            pass
        else:
            raise RuntimeError(f'unexpected Content-Encoding: {content_encoding!r}')
        self.data = data
        self.headers = response.headers

    @property
    def json(self):
        return json.loads(self.data, object_hook=DictProxy)

wget = UserAgent.get

def wget_json(url):
    return wget(url).json

class DictProxy():

    def __init__(self, d):
        self._d = d

    def __getattr__(self, attr):
        return self._d[attr]

def fmt_url(url):
    if sys.stdout.isatty():
        return re.sub('(.)', r'_\b\1', url)
    return url

def fmt_user(account):
    return f'{account.display_name} <{fmt_url(account.url)}>'.lstrip()

def fmt_date(d):
    d = re.sub(r'[.]\d+', '', d)
    d = d.replace('T', ' ')
    return d

class HTMLParser(html.parser.HTMLParser):

    def __init__(self):
        super().__init__()
        class state:
            paras = []
            text = ''
            a_text = ''
            a_href = None
            a_depth = 0
        self.z_state = state

    def handle_starttag(self, tag, attrs):
        st = self.z_state
        if tag == 'p':
            while st.a_depth > 0:
                self.handle_endtag('a')
            if st.text:
                st.paras += [st.text]
                st.text = ''
            return
        if tag == 'br':
            if st.a_depth > 0:
                st.a_text += ' '
            else:
                st.text += '\n'
            return
        if tag == 'a':
            if st.a_depth == 0:
                st.a_href = dict(attrs).get('href', '')
            st.a_depth += 1
            return

    def handle_endtag(self, tag):
        st = self.z_state
        if tag == 'a':
            if st.a_depth > 0:
                st.a_depth -= 1
            if st.a_depth == 0:
                text = ''
                if st.a_text != st.a_href:
                    text = f'[{st.a_text}]'
                st.text += f'{text}\N{STX}{st.a_href}\N{ETX}'
                st.a_href = ''
                st.a_text = ''
            return

    def handle_data(self, data):
        st = self.z_state
        data = re.sub('[\N{STX}\N{ETX}]', '\N{REPLACEMENT CHARACTER}', data)
        data = re.sub(r'\s+', ' ', data)
        if st.a_depth > 0:
            st.a_text += data
        else:
            st.text += data

    def close(self):
        self.handle_starttag('p', {})
        super().close()

    if sys.version_info < (3, 10):
        def error(self, message):
            # hopefully not reachable
            raise RuntimeError(message)

def fmt_html(data):
    parser = HTMLParser()
    parser.feed(data)
    parser.close()
    lines = []
    for para in parser.z_state.paras:
        for line in para.splitlines():
            lines += wrap_text(line)
        lines += ['']
    text = str.join('\n', lines)
    def repl(match):
        url = match.group(1)
        url = fmt_url(url)
        return f'<{url}>'
    text = re.sub('\N{STX}(.*?)\N{ETX}', repl, text, flags=re.DOTALL)
    return text

class URLParser():

    def __init__(self, *templates):
        self.templates = []
        self._groups = set()
        self._regexps = []
        for template in templates:
            self._add_template(f'https://DOMAIN/{template}')

    def _add_template(self, template):
        self.templates += [template]
        group2regexp = dict(
            domain='[^/]+',
            user='[^/]+',
            ident='[0-9]+',
        )
        def repl(match):
            s = match.group()
            if match.start() == 0 and s == 'https':
                return s
            if s.isupper():
                group = s.lower()
                if group == 'nnnnnn':
                    group = 'ident'
                regexp = group2regexp[group]
            else:
                group = s
                regexp = re.escape(s)
            self._groups.add(group)
            return f'(?P<{group}>{regexp})'
        regexp = re.sub(r'\w+', repl, template)
        regexp = re.compile(regexp)
        self._regexps += [regexp]

    def parse(self, url):
        for regexp in self._regexps:
            match = re.fullmatch(regexp, url)
            if match is not None:
                break
        else:
            return None
        data = {group: None for group in self._groups}
        data.update(match.groupdict())
        return types.SimpleNamespace(**data)

def pint(s):
    n = int(s)
    if n > 0:
        return n
    raise ValueError
pint.__name__ = 'positive int'

def xmain():
    url_parser = URLParser(
        'statuses/NNNNNN',
        '@USER/NNNNNN',
        '@USER/NNNNNN/embed',
        '@USER',
        '@USER/with_replies',
        '@USER/media',
    )
    ap = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)
    if sys.version_info < (3, 10):
        # https://bugs.python.org/issue9694
        ap._optionals.title = 'options'  # pylint: disable=protected-access
    default_limit = 40
    ap.add_argument('--limit', metavar='N', type=pint, default=default_limit,
        help=f'request at most N posts (default: {default_limit})'
    )
    ap.add_argument('--debug-http', action='store_true', help=argparse.SUPPRESS)
    ap.add_argument('url', metavar='URL', help=str.join('\n', url_parser.templates))
    opts = ap.parse_args()
    if opts.debug_http:
        UserAgent.debug_level = 1
    url, _ = urllib.parse.urldefrag(opts.url)
    match = url_parser.parse(url)
    if match is None:
        ap.error('unsupported URL')
    sys.stdout.flush()
    with StdOut.install():
        if match.ident is None:
            process_user(match.domain, match.user,
                replies=bool(match.with_replies),
                media=bool(match.media),
                limit=opts.limit,
            )
        else:
            process_status(match.domain, match.ident,
                context=(opts.limit > 1 and not match.embed),
            )

def parse_links(s):
    data = {}
    regexp = re.compile(r'<([^>]+)>; rel="(\w+)"(?:, |\Z)')
    i = 0
    while i < len(s):
        match = regexp.match(s, i)
        if match is None:
            raise RuntimeError(f'cannot parse Link header field: {s!r}')
        (value, key) = match.groups()
        data[key] = value
        i = match.end()
    return data

def process_user(domain, account, *, replies=False, media=False, limit=1e999):
    api = f'https://{domain}/api/v1'
    q_account = urllib.parse.quote(account)
    user = wget_json(f'{api}/accounts/lookup?acct={q_account}')
    print('User:', fmt_user(user))
    if user.note:
        print()
        print(fmt_html(user.note))
    base_url = url = f'{api}/accounts/{user.id}/statuses'
    statuses = wget_json(f'{url}?pinned=true')
    print_statuses(statuses, pinned=True)
    limit -= len(statuses)
    page_limit = 40
    url = base_url
    if media:
        url += '?only_media=true'
    else:
        no_replies = str(not replies).lower()
        url += f'?exclude_replies={no_replies}'
    url += f'&limit={min(limit, page_limit)}'
    while limit > 0:
        response = wget(url)
        statuses = response.json
        print_statuses(statuses, domain=domain)
        limit -= len(statuses)
        links = response.headers.get('Link', '')
        links = parse_links(links)
        prev_url = links.get('next')
        if prev_url is None:
            break
        if not url.startswith(f'{api}/'):
            raise RuntimeError(f'suspicious Link URL: {prev_url!r}')
        url = re.sub(
            r'(?<=[?&]limit=)\d+(?=&|\Z)',
            str(min(limit, page_limit)),
            prev_url
        )

def process_status(domain, ident, *, context=True):
    api = f'https://{domain}/api/v1'
    post = wget_json(f'{api}/statuses/{ident}')
    print_post(post, domain=domain)
    if context:
        context = wget_json(f'{api}/statuses/{ident}/context')
        print_statuses(context.descendants)

def print_statuses(statuses, *, pinned=False, domain=None):
    for post in statuses:
        print()
        print('-' * text_width)
        print()
        print_post(post, pinned=pinned, domain=domain)

def normalize_lang(lang):
    if lang is None:
        return 'en'
    if lang.startswith('en-'):
        return 'en'
    return lang

def print_post(post, *, pinned=False, domain=None):
    print('Location:', fmt_url(post.url))
    if post.in_reply_to_id and domain:
        print('In-Reply-To:', fmt_url(f'https://{domain}/statuses/{post.in_reply_to_id}'))
    if pinned:
        print('Pinned: yes')
    print('From:', fmt_user(post.account))
    print('Date:', fmt_date(post.created_at))
    if normalize_lang(post.language) != 'en':
        print('Language:', post.language)
    if post.reblog:
        print('Reblog: yes')
    print()
    if post.reblog:
        print_post(post.reblog)
    else:
        text = fmt_html(post.content)
        print(text)
    print()
    for att in post.media_attachments or ():
        print('*', fmt_url(att.url))
        print()
        text = att.description or ''
        text = wrap_text(text, indent='  ')
        for line in text:
            print(line)
        print()

def wrap_text(text, indent=''):
    text = text.splitlines()
    for line in text:
        line = textwrap.wrap(line,
            width=text_width,
            initial_indent=indent,
            subsequent_indent=indent,
            break_long_words=False,
        )
        yield str.join('\n', line)

def main():
    try:
        xmain()
    except BrokenPipeError:
        signal.signal(signal.SIGPIPE, signal.SIG_DFL)
        os.kill(os.getpid(), signal.SIGPIPE)
        raise

if __name__ == '__main__':
    main()

# vim:ts=4 sts=4 sw=4 et
